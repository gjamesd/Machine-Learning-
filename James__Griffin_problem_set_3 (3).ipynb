{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "James _Griffin_problem_set_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGmkaj1GMPL0",
        "colab_type": "text"
      },
      "source": [
        "# Problem Set 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1EZUyhhMPMR",
        "colab_type": "code",
        "outputId": "0452e73b-90b1-4538-da18-c215d42c88fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgW1IpUqM25c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## importing cuz free trade lol bad joke \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pprint import pprint\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import tree"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gmPXNkt0ndE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH0aM1yWMPMX",
        "colab_type": "text"
      },
      "source": [
        "For the next problem we will be using the `Carseats` data set that is available on learningsuite. Load the data and convert the text variables into dummies so that we can use them in the data. Pandas has a function called `get_dummies` that you might want to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj5C3JUPMPMY",
        "colab_type": "code",
        "outputId": "7f38aaff-332a-4b24-a77b-955120fde392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "car = pd.read_csv(\"/content/drive/My Drive/Carseats.csv\")\n",
        "car.head()\n",
        "#ok so it looks like I have dummy variables in Urban, US, ShelveLoc\n",
        "\n",
        "dummies = pd.get_dummies(car[[\"Urban\", \"US\", \"ShelveLoc\"]])\n",
        "dummies.head()\n",
        "car = pd.concat([car, dummies], axis = 1) #axis = 1 denotes columns vs rows\n",
        "car.head()\n",
        "#excelent now I can delete my orriginal shelveloc, us and urban columns \n",
        "\n",
        "car.drop(axis = 1, columns = [\"Urban\", \"US\", \"ShelveLoc\"], inplace = True)\n",
        "car.columns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Sales', 'CompPrice', 'Income', 'Advertising',\n",
              "       'Population', 'Price', 'Age', 'Education', 'Urban_No', 'Urban_Yes',\n",
              "       'US_No', 'US_Yes', 'ShelveLoc_Bad', 'ShelveLoc_Good',\n",
              "       'ShelveLoc_Medium'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XHW4w3gMPMb",
        "colab_type": "text"
      },
      "source": [
        "Now that the data has only numeric columns, we can proceed to the analysis.  \n",
        "Use `Sales` as the outcome variable  \n",
        "(a) Split the data set into a training set and a test set.  \n",
        "(b) Fit a regression tree to the training set.  What test MSE do you obtain?  \n",
        "(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE? Plot a tree with a depth of 3, and interpret the results.  \n",
        "(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Look at the feature importances attribute of your model object to determine which variables are most important.  \n",
        "(e) Use random forests to analyze this data. What test MSE do you obtain? Look at the feature importances attribute of your model objec function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSfcQAJfMPMc",
        "colab_type": "code",
        "outputId": "80dc6648-57ea-47fa-de86-e2ed3764bba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# A)\n",
        "y = car.Sales\n",
        "#selecting all data except for Sales \n",
        "car = car.loc[:, car.columns != \"Sales\"]\n",
        "car.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'CompPrice', 'Income', 'Advertising', 'Population',\n",
              "       'Price', 'Age', 'Education', 'Urban_No', 'Urban_Yes', 'US_No', 'US_Yes',\n",
              "       'ShelveLoc_Bad', 'ShelveLoc_Good', 'ShelveLoc_Medium'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLhfYQA0RN34",
        "colab_type": "code",
        "outputId": "1615d509-9768-44ae-944c-8dbeb3f6c502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(car, y,\n",
        "                                                   test_size = 0.33,\n",
        "                                                   random_state = 42)\n",
        "#print(y_train)\n",
        "#decision trees -- gonna do a random forest cuz I think thats more effective.. \n",
        "# hope that is ok  \n",
        "#(b) Fit a regression tree to the training set. What test MSE do you obtain?\n",
        "'''creating an object from the class'''\n",
        "dt = DecisionTreeRegressor(random_state = 42)\n",
        "#fitting the random forest to our training \n",
        "dt.fit(X_train, y_train)\n",
        "#creating our test predictions of which node they will be under \n",
        "test_predictions = dt.predict(X_test)\n",
        "#getting test mse \n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "#printing \n",
        "print(\"Test MSE: {}\".format(test_mse))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test MSE: 4.335544696969697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8mKiIh7nyLe",
        "colab_type": "code",
        "outputId": "886f1dca-229d-4f9a-f06e-831a77aae04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "'''c) Use cross-validation in order to determine the optimal level of tree \n",
        "complexity. Does pruning the tree improve the test MSE? Plot a tree with a \n",
        "depth of 3, and interpret the results.'''\n",
        "\n",
        "#creating a grid to search over\n",
        "                        #different scoreing types \n",
        "param_grid = {\"criterion\": [\"mse\", \"mae\"], \n",
        "              \"min_samples_split\": [10, 20, 40], #sample splits \n",
        "              \"max_depth\": [2, 6, 8, 10],\n",
        "              \"min_samples_leaf\": [20, 40, 60, 80, 100],\n",
        "              \"max_leaf_nodes\": [5, 10, 15, 20, 25],\n",
        "              }\n",
        "#creatubg grid search over the range of the tree lengths \n",
        "gs = GridSearchCV(dt, \n",
        "                  param_grid, \n",
        "                  cv=5)\n",
        "#fitting\n",
        "gs.fit(X_train, y_train)\n",
        "#finding the best parameters \n",
        "print(\"best parameters\")\n",
        "print(gs.best_params_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best parameters\n",
            "{'criterion': 'mse', 'max_depth': 6, 'max_leaf_nodes': 10, 'min_samples_leaf': 20, 'min_samples_split': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgYtJaE-kjKV",
        "colab_type": "code",
        "outputId": "787f618e-e709-446e-c847-4a10bd862d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#getting test results \n",
        "gs_preds = gs.predict(X_test)\n",
        "gs_mse = mean_squared_error(gs_preds, y_test)\n",
        "print(\"GS MSE: {}\".format(gs_mse))\n",
        "\n",
        "'''#interesting, so our grid search did worse but only slightly, \n",
        "what probably happened is it over fitted in training so it preformed\n",
        "worse out of sample, but I am surprised that it is only a few decimal pts\n",
        "off ''' "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GS MSE: 4.5384443125568525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#interesting, so our grid search did worse but only slightly, \\nwhat probably happened is it over fitted in training so it preformed\\nworse out of sample, but I am surprised that it is only a few decimal pts\\noff '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Hf-WkolHWj",
        "colab_type": "code",
        "outputId": "29b46575-fae1-431b-e051-9e7078d0ca75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "export_graphviz(dt, \n",
        "                out_file=\"tree.dot\",\n",
        "                feature_names = car.columns,\n",
        "                max_depth = 3)\n",
        "\n",
        "import graphviz\n",
        "\n",
        "with open(\"tree.dot\") as f:\n",
        "    dot_graph = f.read()\n",
        "display(graphviz.Source(dot_graph))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<graphviz.files.Source at 0x7fa772490f98>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"1210pt\" height=\"460pt\"\n viewBox=\"0.00 0.00 1210.00 460.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 456)\">\n<title>Tree</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-456 1206,-456 1206,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"689.5,-452 528.5,-452 528.5,-384 689.5,-384 689.5,-452\"/>\n<text text-anchor=\"middle\" x=\"609\" y=\"-436.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ShelveLoc_Good &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"609\" y=\"-421.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 7.758</text>\n<text text-anchor=\"middle\" x=\"609\" y=\"-406.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 268</text>\n<text text-anchor=\"middle\" x=\"609\" y=\"-391.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 7.298</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"504.5,-348 401.5,-348 401.5,-280 504.5,-280 504.5,-348\"/>\n<text text-anchor=\"middle\" x=\"453\" y=\"-332.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Price &lt;= 105.5</text>\n<text text-anchor=\"middle\" x=\"453\" y=\"-317.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 6.291</text>\n<text text-anchor=\"middle\" x=\"453\" y=\"-302.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 215</text>\n<text text-anchor=\"middle\" x=\"453\" y=\"-287.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 6.678</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M557.9198,-383.9465C543.5092,-374.3395 527.7014,-363.8009 512.8272,-353.8848\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"514.457,-350.7649 504.195,-348.13 510.574,-356.5892 514.457,-350.7649\"/>\n<text text-anchor=\"middle\" x=\"509.0979\" y=\"-368.9445\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 430 -->\n<g id=\"node17\" class=\"node\">\n<title>430</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"816.5,-348 713.5,-348 713.5,-280 816.5,-280 816.5,-348\"/>\n<text text-anchor=\"middle\" x=\"765\" y=\"-332.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Price &lt;= 135.0</text>\n<text text-anchor=\"middle\" x=\"765\" y=\"-317.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 5.831</text>\n<text text-anchor=\"middle\" x=\"765\" y=\"-302.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 53</text>\n<text text-anchor=\"middle\" x=\"765\" y=\"-287.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.812</text>\n</g>\n<!-- 0&#45;&gt;430 -->\n<g id=\"edge16\" class=\"edge\">\n<title>0&#45;&gt;430</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M660.0802,-383.9465C674.4908,-374.3395 690.2986,-363.8009 705.1728,-353.8848\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"707.426,-356.5892 713.805,-348.13 703.543,-350.7649 707.426,-356.5892\"/>\n<text text-anchor=\"middle\" x=\"708.9021\" y=\"-368.9445\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"277.5,-244 180.5,-244 180.5,-176 277.5,-176 277.5,-244\"/>\n<text text-anchor=\"middle\" x=\"229\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Age &lt;= 48.5</text>\n<text text-anchor=\"middle\" x=\"229\" y=\"-213.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 5.562</text>\n<text text-anchor=\"middle\" x=\"229\" y=\"-198.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 69</text>\n<text text-anchor=\"middle\" x=\"229\" y=\"-183.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 8.352</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M401.3286,-290.0097C367.1702,-274.1505 322.3406,-253.3367 286.9676,-236.9135\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"288.2695,-233.6592 277.7255,-232.6225 285.3217,-240.0082 288.2695,-233.6592\"/>\n</g>\n<!-- 139 -->\n<g id=\"node10\" class=\"node\">\n<title>139</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"529,-244 377,-244 377,-176 529,-176 529,-244\"/>\n<text text-anchor=\"middle\" x=\"453\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ShelveLoc_Bad &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"453\" y=\"-213.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 4.685</text>\n<text text-anchor=\"middle\" x=\"453\" y=\"-198.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 146</text>\n<text text-anchor=\"middle\" x=\"453\" y=\"-183.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 5.886</text>\n</g>\n<!-- 1&#45;&gt;139 -->\n<g id=\"edge9\" class=\"edge\">\n<title>1&#45;&gt;139</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M453,-279.9465C453,-271.776 453,-262.9318 453,-254.3697\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"456.5001,-254.13 453,-244.13 449.5001,-254.13 456.5001,-254.13\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"152,-140 0,-140 0,-72 152,-72 152,-140\"/>\n<text text-anchor=\"middle\" x=\"76\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ShelveLoc_Bad &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"76\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 3.224</text>\n<text text-anchor=\"middle\" x=\"76\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 29</text>\n<text text-anchor=\"middle\" x=\"76\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 9.734</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M180.1692,-176.8078C165.7673,-167.0183 149.8533,-156.2009 134.8898,-146.0297\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.4459,-142.8554 126.208,-140.1283 132.5107,-148.6446 136.4459,-142.8554\"/>\n</g>\n<!-- 60 -->\n<g id=\"node7\" class=\"node\">\n<title>60</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"287.5,-140 170.5,-140 170.5,-72 287.5,-72 287.5,-140\"/>\n<text text-anchor=\"middle\" x=\"229\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Income &lt;= 105.5</text>\n<text text-anchor=\"middle\" x=\"229\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 4.867</text>\n<text text-anchor=\"middle\" x=\"229\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 40</text>\n<text text-anchor=\"middle\" x=\"229\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 7.35</text>\n</g>\n<!-- 2&#45;&gt;60 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;60</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229,-175.9465C229,-167.776 229,-158.9318 229,-150.3697\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"232.5001,-150.13 229,-140.13 225.5001,-150.13 232.5001,-150.13\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"90,-36 36,-36 36,0 90,0 90,-36\"/>\n<text text-anchor=\"middle\" x=\"63\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.9739,-71.9769C69.7219,-63.5023 68.393,-54.5065 67.1864,-46.3388\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"70.6114,-45.5731 65.6874,-36.192 63.6865,-46.5961 70.6114,-45.5731\"/>\n</g>\n<!-- 43 -->\n<g id=\"node6\" class=\"node\">\n<title>43</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"162,-36 108,-36 108,0 162,0 162,-36\"/>\n<text text-anchor=\"middle\" x=\"135\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 3&#45;&gt;43 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;43</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M98.8109,-71.9769C104.8758,-62.931 111.3388,-53.2913 117.0998,-44.6986\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.1414,-46.447 122.8031,-36.192 114.3272,-42.5488 120.1414,-46.447\"/>\n</g>\n<!-- 61 -->\n<g id=\"node8\" class=\"node\">\n<title>61</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"245,-36 191,-36 191,0 245,0 245,-36\"/>\n<text text-anchor=\"middle\" x=\"218\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 60&#45;&gt;61 -->\n<g id=\"edge7\" class=\"edge\">\n<title>60&#45;&gt;61</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M224.7471,-71.9769C223.6878,-63.5023 222.5633,-54.5065 221.5423,-46.3388\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"224.9874,-45.6806 220.274,-36.192 218.0415,-46.5489 224.9874,-45.6806\"/>\n</g>\n<!-- 128 -->\n<g id=\"node9\" class=\"node\">\n<title>128</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"317,-36 263,-36 263,0 317,0 317,-36\"/>\n<text text-anchor=\"middle\" x=\"290\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 60&#45;&gt;128 -->\n<g id=\"edge8\" class=\"edge\">\n<title>60&#45;&gt;128</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M252.5842,-71.9769C258.9206,-62.8358 265.6775,-53.0883 271.6809,-44.4276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"274.5692,-46.4045 277.3897,-36.192 268.8162,-42.4166 274.5692,-46.4045\"/>\n</g>\n<!-- 140 -->\n<g id=\"node11\" class=\"node\">\n<title>140</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"444.5,-140 305.5,-140 305.5,-72 444.5,-72 444.5,-140\"/>\n<text text-anchor=\"middle\" x=\"375\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CompPrice &lt;= 124.5</text>\n<text text-anchor=\"middle\" x=\"375\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 4.211</text>\n<text text-anchor=\"middle\" x=\"375\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 104</text>\n<text text-anchor=\"middle\" x=\"375\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 6.437</text>\n</g>\n<!-- 139&#45;&gt;140 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139&#45;&gt;140</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M427.4599,-175.9465C420.8607,-167.1475 413.6754,-157.5672 406.7995,-148.3993\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"409.3975,-146.03 400.5975,-140.13 403.7975,-150.23 409.3975,-146.03\"/>\n</g>\n<!-- 347 -->\n<g id=\"node14\" class=\"node\">\n<title>347</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"601.5,-140 462.5,-140 462.5,-72 601.5,-72 601.5,-140\"/>\n<text text-anchor=\"middle\" x=\"532\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CompPrice &lt;= 151.0</text>\n<text text-anchor=\"middle\" x=\"532\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 3.245</text>\n<text text-anchor=\"middle\" x=\"532\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 42</text>\n<text text-anchor=\"middle\" x=\"532\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 4.522</text>\n</g>\n<!-- 139&#45;&gt;347 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139&#45;&gt;347</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M478.8676,-175.9465C485.6196,-167.0578 492.9774,-157.3716 500.0059,-148.1188\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"502.8125,-150.2102 506.0743,-140.13 497.2383,-145.976 502.8125,-150.2102\"/>\n</g>\n<!-- 141 -->\n<g id=\"node12\" class=\"node\">\n<title>141</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"395,-36 341,-36 341,0 395,0 395,-36\"/>\n<text text-anchor=\"middle\" x=\"368\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 140&#45;&gt;141 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140&#45;&gt;141</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M372.2936,-71.9769C371.6195,-63.5023 370.9039,-54.5065 370.2542,-46.3388\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"373.7291,-45.8829 369.4471,-36.192 366.7511,-46.438 373.7291,-45.8829\"/>\n</g>\n<!-- 220 -->\n<g id=\"node13\" class=\"node\">\n<title>220</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"467,-36 413,-36 413,0 467,0 467,-36\"/>\n<text text-anchor=\"middle\" x=\"440\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 140&#45;&gt;220 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140&#45;&gt;220</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M400.1307,-71.9769C406.8826,-62.8358 414.0825,-53.0883 420.4796,-44.4276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"423.4367,-46.3151 426.5628,-36.192 417.8061,-42.1561 423.4367,-46.3151\"/>\n</g>\n<!-- 348 -->\n<g id=\"node15\" class=\"node\">\n<title>348</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"547,-36 493,-36 493,0 547,0 547,-36\"/>\n<text text-anchor=\"middle\" x=\"520\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 347&#45;&gt;348 -->\n<g id=\"edge14\" class=\"edge\">\n<title>347&#45;&gt;348</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M527.3605,-71.9769C526.2049,-63.5023 524.9782,-54.5065 523.8644,-46.3388\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"527.2998,-45.6273 522.4807,-36.192 520.364,-46.5732 527.2998,-45.6273\"/>\n</g>\n<!-- 425 -->\n<g id=\"node16\" class=\"node\">\n<title>425</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"619,-36 565,-36 565,0 619,0 619,-36\"/>\n<text text-anchor=\"middle\" x=\"592\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 347&#45;&gt;425 -->\n<g id=\"edge15\" class=\"edge\">\n<title>347&#45;&gt;425</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M555.1976,-71.9769C561.3652,-62.931 567.9378,-53.2913 573.7964,-44.6986\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"576.8548,-46.4259 579.5964,-36.192 571.0712,-42.4825 576.8548,-46.4259\"/>\n</g>\n<!-- 431 -->\n<g id=\"node18\" class=\"node\">\n<title>431</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"813.5,-244 716.5,-244 716.5,-176 813.5,-176 813.5,-244\"/>\n<text text-anchor=\"middle\" x=\"765\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Price &lt;= 74.5</text>\n<text text-anchor=\"middle\" x=\"765\" y=\"-213.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 3.657</text>\n<text text-anchor=\"middle\" x=\"765\" y=\"-198.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 41</text>\n<text text-anchor=\"middle\" x=\"765\" y=\"-183.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.63</text>\n</g>\n<!-- 430&#45;&gt;431 -->\n<g id=\"edge17\" class=\"edge\">\n<title>430&#45;&gt;431</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M765,-279.9465C765,-271.776 765,-262.9318 765,-254.3697\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"768.5001,-254.13 765,-244.13 761.5001,-254.13 768.5001,-254.13\"/>\n</g>\n<!-- 512 -->\n<g id=\"node25\" class=\"node\">\n<title>512</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1055,-244 909,-244 909,-176 1055,-176 1055,-244\"/>\n<text text-anchor=\"middle\" x=\"982\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Unnamed: 0 &lt;= 334.5</text>\n<text text-anchor=\"middle\" x=\"982\" y=\"-213.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 3.155</text>\n<text text-anchor=\"middle\" x=\"982\" y=\"-198.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 12</text>\n<text text-anchor=\"middle\" x=\"982\" y=\"-183.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 7.017</text>\n</g>\n<!-- 430&#45;&gt;512 -->\n<g id=\"edge24\" class=\"edge\">\n<title>430&#45;&gt;512</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M816.7012,-289.2215C842.1526,-277.0236 873.4111,-262.0426 901.8692,-248.4037\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"903.4943,-251.5061 910.9995,-244.0279 900.469,-245.1936 903.4943,-251.5061\"/>\n</g>\n<!-- 432 -->\n<g id=\"node19\" class=\"node\">\n<title>432</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"756.5,-140 619.5,-140 619.5,-72 756.5,-72 756.5,-140\"/>\n<text text-anchor=\"middle\" x=\"688\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Population &lt;= 135.0</text>\n<text text-anchor=\"middle\" x=\"688\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 1.171</text>\n<text text-anchor=\"middle\" x=\"688\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n<text text-anchor=\"middle\" x=\"688\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 14.327</text>\n</g>\n<!-- 431&#45;&gt;432 -->\n<g id=\"edge18\" class=\"edge\">\n<title>431&#45;&gt;432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M739.7873,-175.9465C733.2727,-167.1475 726.1795,-157.5672 719.3918,-148.3993\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"722.0327,-146.0843 713.2693,-140.13 716.4069,-150.2496 722.0327,-146.0843\"/>\n</g>\n<!-- 437 -->\n<g id=\"node22\" class=\"node\">\n<title>437</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"909,-140 775,-140 775,-72 909,-72 909,-140\"/>\n<text text-anchor=\"middle\" x=\"842\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Advertising &lt;= 11.5</text>\n<text text-anchor=\"middle\" x=\"842\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 2.69</text>\n<text text-anchor=\"middle\" x=\"842\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 38</text>\n<text text-anchor=\"middle\" x=\"842\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 10.338</text>\n</g>\n<!-- 431&#45;&gt;437 -->\n<g id=\"edge21\" class=\"edge\">\n<title>431&#45;&gt;437</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M790.2127,-175.9465C796.7273,-167.1475 803.8205,-157.5672 810.6082,-148.3993\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"813.5931,-150.2496 816.7307,-140.13 807.9673,-146.0843 813.5931,-150.2496\"/>\n</g>\n<!-- 433 -->\n<g id=\"node20\" class=\"node\">\n<title>433</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"695,-36 641,-36 641,0 695,0 695,-36\"/>\n<text text-anchor=\"middle\" x=\"668\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 432&#45;&gt;433 -->\n<g id=\"edge19\" class=\"edge\">\n<title>432&#45;&gt;433</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M680.2675,-71.9769C678.3198,-63.4071 676.251,-54.3043 674.3781,-46.0638\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"677.7638,-45.1676 672.1345,-36.192 670.9379,-46.719 677.7638,-45.1676\"/>\n</g>\n<!-- 434 -->\n<g id=\"node21\" class=\"node\">\n<title>434</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"767,-36 713,-36 713,0 767,0 767,-36\"/>\n<text text-anchor=\"middle\" x=\"740\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 432&#45;&gt;434 -->\n<g id=\"edge20\" class=\"edge\">\n<title>432&#45;&gt;434</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M708.1046,-71.9769C713.3936,-63.0262 719.0262,-53.4941 724.063,-44.9703\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"727.1761,-46.5818 729.2502,-36.192 721.1496,-43.0207 727.1761,-46.5818\"/>\n</g>\n<!-- 438 -->\n<g id=\"node23\" class=\"node\">\n<title>438</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"841,-36 787,-36 787,0 841,0 841,-36\"/>\n<text text-anchor=\"middle\" x=\"814\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 437&#45;&gt;438 -->\n<g id=\"edge22\" class=\"edge\">\n<title>437&#45;&gt;438</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M831.1745,-71.9769C828.4174,-63.3119 825.487,-54.102 822.8421,-45.7894\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"826.1557,-44.66 819.7883,-36.192 819.4852,-46.7825 826.1557,-44.66\"/>\n</g>\n<!-- 487 -->\n<g id=\"node24\" class=\"node\">\n<title>487</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"913,-36 859,-36 859,0 913,0 913,-36\"/>\n<text text-anchor=\"middle\" x=\"886\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 437&#45;&gt;487 -->\n<g id=\"edge23\" class=\"edge\">\n<title>437&#45;&gt;487</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M859.0115,-71.9769C863.4393,-63.1215 868.1516,-53.6969 872.3787,-45.2427\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"875.5623,-46.7015 876.904,-36.192 869.3013,-43.571 875.5623,-46.7015\"/>\n</g>\n<!-- 513 -->\n<g id=\"node26\" class=\"node\">\n<title>513</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1037,-140 927,-140 927,-72 1037,-72 1037,-140\"/>\n<text text-anchor=\"middle\" x=\"982\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Income &lt;= 46.0</text>\n<text text-anchor=\"middle\" x=\"982\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 1.8</text>\n<text text-anchor=\"middle\" x=\"982\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 10</text>\n<text text-anchor=\"middle\" x=\"982\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 7.581</text>\n</g>\n<!-- 512&#45;&gt;513 -->\n<g id=\"edge25\" class=\"edge\">\n<title>512&#45;&gt;513</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M982,-175.9465C982,-167.776 982,-158.9318 982,-150.3697\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"985.5001,-150.13 982,-140.13 978.5001,-150.13 985.5001,-150.13\"/>\n</g>\n<!-- 532 -->\n<g id=\"node29\" class=\"node\">\n<title>532</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1152.5,-140 1055.5,-140 1055.5,-72 1152.5,-72 1152.5,-140\"/>\n<text text-anchor=\"middle\" x=\"1104\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Age &lt;= 75.5</text>\n<text text-anchor=\"middle\" x=\"1104\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mse = 0.378</text>\n<text text-anchor=\"middle\" x=\"1104\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n<text text-anchor=\"middle\" x=\"1104\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = 4.195</text>\n</g>\n<!-- 512&#45;&gt;532 -->\n<g id=\"edge28\" class=\"edge\">\n<title>512&#45;&gt;532</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1021.9474,-175.9465C1032.9012,-166.6088 1044.8873,-156.3911 1056.232,-146.7203\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1058.6233,-149.2809 1063.9629,-140.13 1054.0821,-143.9538 1058.6233,-149.2809\"/>\n</g>\n<!-- 514 -->\n<g id=\"node27\" class=\"node\">\n<title>514</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"986,-36 932,-36 932,0 986,0 986,-36\"/>\n<text text-anchor=\"middle\" x=\"959\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 513&#45;&gt;514 -->\n<g id=\"edge26\" class=\"edge\">\n<title>513&#45;&gt;514</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M973.1076,-71.9769C970.8678,-63.4071 968.4886,-54.3043 966.3349,-46.0638\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"969.6697,-44.9819 963.7547,-36.192 962.8972,-46.752 969.6697,-44.9819\"/>\n</g>\n<!-- 525 -->\n<g id=\"node28\" class=\"node\">\n<title>525</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1058,-36 1004,-36 1004,0 1058,0 1058,-36\"/>\n<text text-anchor=\"middle\" x=\"1031\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 513&#45;&gt;525 -->\n<g id=\"edge27\" class=\"edge\">\n<title>513&#45;&gt;525</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1000.9447,-71.9769C1005.9286,-63.0262 1011.2362,-53.4941 1015.9825,-44.9703\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1019.0634,-46.6316 1020.8704,-36.192 1012.9476,-43.2261 1019.0634,-46.6316\"/>\n</g>\n<!-- 533 -->\n<g id=\"node30\" class=\"node\">\n<title>533</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1130,-36 1076,-36 1076,0 1130,0 1130,-36\"/>\n<text text-anchor=\"middle\" x=\"1103\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 532&#45;&gt;533 -->\n<g id=\"edge29\" class=\"edge\">\n<title>532&#45;&gt;533</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1103.6134,-71.9769C1103.5171,-63.5023 1103.4148,-54.5065 1103.322,-46.3388\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1106.8202,-46.1515 1103.2067,-36.192 1099.8207,-46.2311 1106.8202,-46.1515\"/>\n</g>\n<!-- 534 -->\n<g id=\"node31\" class=\"node\">\n<title>534</title>\n<polygon fill=\"none\" stroke=\"#000000\" points=\"1202,-36 1148,-36 1148,0 1202,0 1202,-36\"/>\n<text text-anchor=\"middle\" x=\"1175\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(...)</text>\n</g>\n<!-- 532&#45;&gt;534 -->\n<g id=\"edge30\" class=\"edge\">\n<title>532&#45;&gt;534</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M1131.4505,-71.9769C1138.9025,-62.7406 1146.8541,-52.8851 1153.8958,-44.1573\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1156.7671,-46.1725 1160.3224,-36.192 1151.3191,-41.777 1156.7671,-46.1725\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNRGAv5rngl1",
        "colab_type": "code",
        "outputId": "2f603b82-3f79-4f82-e5c0-31bacabf2f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "'''(d) Use the bagging approach in order to analyze this data. \n",
        "What test MSE do you obtain? Look at the feature importances attribute of your\n",
        " model object to determine which variables are most important.'''\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "bagging = BaggingRegressor()\n",
        "#{'criterion': 'mse', 'max_depth': 6, 'max_leaf_nodes': 10, 'min_samples_leaf': 20, 'min_samples_split': 10}\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_predictions = bagging.predict(X_test)\n",
        "bag_test_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "#printing MSE\n",
        "print(\"Bag Test MSE: {}\".format(bag_test_mse))\n",
        "# cool looks like the bagging approach helps reduce some varience and get \n",
        "# more accurate test measurements "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag Test MSE: 3.0698711590909094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnv17t3AxgqH",
        "colab_type": "code",
        "outputId": "f57322bd-a967-4621-fd01-a30ff844c1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# #finding feature importance \n",
        "# feature_imp = sorted(list(zip(car.columns, \n",
        "#                               dt.feature_importances_)),\n",
        "#                       key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "# pd.Series([x[1] for x in feature_imp], \n",
        "#           index=[x[0] for x in feature_imp]).plot(kind='bar')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_feature_importances(model):\n",
        "    #the number of features in the column axis of my data frame\n",
        "    n_features = car.shape[1]\n",
        "    #plotting horizontal bar chart \n",
        "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
        "    #column names beside the features \n",
        "    plt.yticks(np.arange(n_features), car.columns)\n",
        "    #labels \n",
        "    plt.xlabel(\"Feature importance\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.ylim(-1, n_features)\n",
        "#plot \n",
        "plot_feature_importances(dt)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAEGCAYAAAA6+K8MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8BRh2tPFsA1CcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNm-zQ9wZ7yI",
        "colab_type": "code",
        "outputId": "823c55b0-957d-4e5c-d83c-ff5488acb057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "'''(e) Use random forests to analyze this data. What test MSE do you obtain? \n",
        "Look at the feature importances attribute of your model objec function to \n",
        "determine which variables are most important.Describe the effect of m, the\n",
        "number of variables considered at each split, on the error rate obtained.'''\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "''' to build a good random forest, you need a good max depth\n",
        "and a good number of trees, and you should also test if your \n",
        "classes are balanced or not, so we will see what are the best \n",
        "parameters for the random forest we will build'''\n",
        "\n",
        "#creating a dictionary with multiple options for the parameters, \n",
        "param_dict = {\"n_estimators\":[100,500,1000,2500,5000],\n",
        "              \"max_depth\":[1,3,5,10], \"criterion\":[\"mse\"]}\n",
        "\n",
        "#creating the random forest classifier \n",
        "rf = RandomForestRegressor()\n",
        "#creating the grid search to search over my parameters and find the best ones \n",
        "gs = GridSearchCV(rf, param_dict, \n",
        "                  n_jobs = -1, verbose = 2)\n",
        "\n",
        "gs.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  2.6min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
              "                                             max_depth=None,\n",
              "                                             max_features='auto',\n",
              "                                             max_leaf_nodes=None,\n",
              "                                             min_impurity_decrease=0.0,\n",
              "                                             min_impurity_split=None,\n",
              "                                             min_samples_leaf=1,\n",
              "                                             min_samples_split=2,\n",
              "                                             min_weight_fraction_leaf=0.0,\n",
              "                                             n_estimators='warn', n_jobs=None,\n",
              "                                             oob_score=False, random_state=None,\n",
              "                                             verbose=0, warm_start=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'criterion': ['mse'], 'max_depth': [1, 3, 5, 10],\n",
              "                         'n_estimators': [100, 500, 1000, 2500, 5000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm0LDJiQdY9s",
        "colab_type": "code",
        "outputId": "daa17d75-5419-4922-976d-b3bc8edbe4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#but interesting to see the best parameters \n",
        "print(\"Best Params: {}\".format(gs.best_params_))\n",
        "print(\"Random forest MSE: {}\".format(gs_mse))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Params: {'criterion': 'mse', 'max_depth': 10, 'n_estimators': 2500}\n",
            "Random forest MSE: 4.5384443125568525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL6LQQOgd-Qy",
        "colab_type": "code",
        "outputId": "78f891ff-9d9a-4314-e585-63dad5884761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "rf.fit(X_train, y_train)\n",
        "plot_feature_importances(rf)\n",
        "\n",
        "'''Cool, so this is almost model also weights price at a really high weight,\n",
        "sheveloc_good also high, but not as high as bagging, age, advertising, \n",
        "shevloc_bad, advertising, and comp price '''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cool, so this is almost model also weights price at a really high weight,\\nsheveloc_good also high, but not as high as bagging, age, advertising, \\nshevloc_bad, advertising, and comp price '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdEAAAEGCAYAAAA6+K8MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_czxRPS4cUe9",
        "colab_type": "code",
        "outputId": "f62551b9-5d6a-4aff-fd91-9b0f3e45b8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>CompPrice</th>\n",
              "      <th>Income</th>\n",
              "      <th>Advertising</th>\n",
              "      <th>Population</th>\n",
              "      <th>Price</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Urban_No</th>\n",
              "      <th>Urban_Yes</th>\n",
              "      <th>US_No</th>\n",
              "      <th>US_Yes</th>\n",
              "      <th>ShelveLoc_Bad</th>\n",
              "      <th>ShelveLoc_Good</th>\n",
              "      <th>ShelveLoc_Medium</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>259</td>\n",
              "      <td>108</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>251</td>\n",
              "      <td>81</td>\n",
              "      <td>72</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>178</td>\n",
              "      <td>138</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>148</td>\n",
              "      <td>94</td>\n",
              "      <td>27</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>120</td>\n",
              "      <td>130</td>\n",
              "      <td>94</td>\n",
              "      <td>8</td>\n",
              "      <td>137</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>195</td>\n",
              "      <td>112</td>\n",
              "      <td>98</td>\n",
              "      <td>18</td>\n",
              "      <td>481</td>\n",
              "      <td>128</td>\n",
              "      <td>45</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>230</td>\n",
              "      <td>98</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>404</td>\n",
              "      <td>72</td>\n",
              "      <td>27</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  CompPrice  ...  ShelveLoc_Good  ShelveLoc_Medium\n",
              "258         259        108  ...               0                 0\n",
              "177         178        138  ...               0                 1\n",
              "119         120        130  ...               0                 1\n",
              "194         195        112  ...               0                 1\n",
              "229         230         98  ...               0                 1\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1p0Fd6zocY",
        "colab_type": "text"
      },
      "source": [
        "*** price is the most important feature in the set followed by ShelveLocGood, which was a dummy variable that we got out at the beginning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRn-s1F9MPMe",
        "colab_type": "text"
      },
      "source": [
        "We now use boosting to predict Log Salary in the `Hitters` data set.  \n",
        "(a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries and use that as the outcome variable. Be sure to not incluide the old salary variable as a predictor  \n",
        "(b) Create a training set consisting of 200 observations, and a test set consisting of the remaining observations.  \n",
        "(c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis. Add a curve with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis. The shrinkage parameter is often referred to as the learning rate   \n",
        "(d) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6. Those are the chapters on OLS and penalized regression.  \n",
        "(e) Which variables appear to be the most important predictors in the boosted model?  \n",
        "(f) The default for base estimator is a Decision Tree with a maximum depth of 3. Is that the optimal depth? Justify your response.\n",
        "(g) Now apply bagging to the training set. What is the test set MSE for this approach?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37CbLYNMPMf",
        "colab_type": "code",
        "outputId": "e16857e7-4d86-4151-a258-2a0ad6944e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "hit = pd.read_csv(\"/content/drive/My Drive/Hitters.csv\")\n",
        "hit.Salary.isnull().sum()\n",
        "#ok so we have 59 null values \n",
        "hit.shape #and that is out of 322 \n",
        "322-59\n",
        "#ok so we should have 263,20 values after we drop all of the nulls \n",
        "hit.dropna(subset = [\"Salary\"], inplace = True)\n",
        "hit.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(263, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5LMlOZ42tvp",
        "colab_type": "code",
        "outputId": "3d3a24ad-4214-4701-cd13-ab1c770f0ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "#creating feature for log salary\n",
        "hit[\"log_sal\"] = np.log(hit[\"Salary\"])\n",
        "hit.head() #sweet \n",
        "\n",
        "#drop the salary column because it will be a perfect indicator of log salary\n",
        "hit.drop(axis = 1, columns = [\"Salary\"])\n",
        "hit.columns"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years', 'CAtBat',\n",
              "       'CHits', 'CHmRun', 'CRuns', 'CRBI', 'CWalks', 'League', 'Division',\n",
              "       'PutOuts', 'Assists', 'Errors', 'Salary', 'NewLeague', 'log_sal'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__8izbUT3dsd",
        "colab_type": "code",
        "outputId": "7f4ce7f1-f92f-4f97-e004-12125bcb2a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "'''(b) Create a training set consisting of 200 observations, \n",
        "and a test set consisting of the remaining observations'''\n",
        "200/263\n",
        "hit.drop(axis = 1, columns = [\"Salary\"], inplace = True)\n",
        "\n",
        "dum = pd.get_dummies(hit[[\"NewLeague\"]])\n",
        "dum.head()\n",
        "hit = pd.concat([hit, dum], axis = 1) #axis = 1 denotes columns vs rows\n",
        "hit.head()\n",
        "\n",
        "\n",
        "#I am actually going to drop league and division from the algorithm, one \n",
        "# I don't think it will be predictive, and 2, new league variables and the \n",
        "#league variables appear to be very very multi-colinear, not that that is \n",
        "#an issue, but the grader will note that that I am thinking of it, and will\n",
        "#be mindful of the possible implications. Will drop division and league\n",
        "#variables and only keep new league variables \n",
        "hit.drop(axis = 1, columns = [\"League\", \"NewLeague\",\n",
        "                              \"Division\"], inplace = True)\n",
        "\n",
        "\n",
        "X = hit.iloc[:, :-1]\n",
        "y = hit[\"log_sal\"]\n",
        "#ok so the data size is 263, so 200.263 is 76% \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(hit, y,\n",
        "                                                   test_size = 0.26,\n",
        "                                                   random_state = 42)\n",
        "X_train.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AtBat</th>\n",
              "      <th>Hits</th>\n",
              "      <th>HmRun</th>\n",
              "      <th>Runs</th>\n",
              "      <th>RBI</th>\n",
              "      <th>Walks</th>\n",
              "      <th>Years</th>\n",
              "      <th>CAtBat</th>\n",
              "      <th>CHits</th>\n",
              "      <th>CHmRun</th>\n",
              "      <th>CRuns</th>\n",
              "      <th>CRBI</th>\n",
              "      <th>CWalks</th>\n",
              "      <th>PutOuts</th>\n",
              "      <th>Assists</th>\n",
              "      <th>Errors</th>\n",
              "      <th>log_sal</th>\n",
              "      <th>NewLeague_A</th>\n",
              "      <th>NewLeague_N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>233</td>\n",
              "      <td>49</td>\n",
              "      <td>2</td>\n",
              "      <td>41</td>\n",
              "      <td>23</td>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>1350</td>\n",
              "      <td>336</td>\n",
              "      <td>7</td>\n",
              "      <td>166</td>\n",
              "      <td>122</td>\n",
              "      <td>106</td>\n",
              "      <td>102</td>\n",
              "      <td>132</td>\n",
              "      <td>10</td>\n",
              "      <td>5.926926</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>341</td>\n",
              "      <td>95</td>\n",
              "      <td>6</td>\n",
              "      <td>48</td>\n",
              "      <td>42</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>2964</td>\n",
              "      <td>808</td>\n",
              "      <td>81</td>\n",
              "      <td>379</td>\n",
              "      <td>428</td>\n",
              "      <td>221</td>\n",
              "      <td>158</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>573</td>\n",
              "      <td>144</td>\n",
              "      <td>9</td>\n",
              "      <td>85</td>\n",
              "      <td>60</td>\n",
              "      <td>78</td>\n",
              "      <td>8</td>\n",
              "      <td>3198</td>\n",
              "      <td>857</td>\n",
              "      <td>97</td>\n",
              "      <td>470</td>\n",
              "      <td>420</td>\n",
              "      <td>332</td>\n",
              "      <td>1314</td>\n",
              "      <td>131</td>\n",
              "      <td>12</td>\n",
              "      <td>6.866933</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>185</td>\n",
              "      <td>40</td>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>524</td>\n",
              "      <td>125</td>\n",
              "      <td>7</td>\n",
              "      <td>58</td>\n",
              "      <td>37</td>\n",
              "      <td>47</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4.499810</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>303</td>\n",
              "      <td>71</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>30</td>\n",
              "      <td>36</td>\n",
              "      <td>3</td>\n",
              "      <td>344</td>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>36</td>\n",
              "      <td>45</td>\n",
              "      <td>468</td>\n",
              "      <td>47</td>\n",
              "      <td>6</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     AtBat  Hits  HmRun  Runs  ...  Errors   log_sal  NewLeague_A  NewLeague_N\n",
              "102    233    49      2    41  ...      10  5.926926            1            0\n",
              "153    341    95      6    48  ...       5  4.605170            0            1\n",
              "320    573   144      9    85  ...      12  6.866933            1            0\n",
              "135    185    40      4    23  ...       2  4.499810            0            1\n",
              "213    303    71      3    18  ...       6  4.605170            0            1\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id4Pho704Jji",
        "colab_type": "code",
        "outputId": "5830bb6a-5776-49f4-fe2c-4f9b0f60544d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''(c) Perform boosting on the training set with 1,000 trees for a range of \n",
        "values of the shrinkage parameter λ. Produce a plot with different shrinkag\n",
        " values on the x-axis and the corresponding training set MSE on the y-axis.\n",
        "  Add a curve with different shrinkage values on the x-axis and the \n",
        "  corresponding test set MSE on the y-axis. The shrinkage parameter is often \n",
        "  referred to as the learning rate'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(c) Perform boosting on the training set with 1,000 trees for a range of \\nvalues of the shrinkage parameter λ. Produce a plot with different shrinkag\\n values on the x-axis and the corresponding training set MSE on the y-axis.\\n  Add a curve with different shrinkage values on the x-axis and the \\n  corresponding test set MSE on the y-axis. The shrinkage parameter is often \\n  referred to as the learning rate'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3QC9Rzq_EV9",
        "colab_type": "code",
        "outputId": "5a192c7f-d7ee-47af-8b88-d6c48b6ccc93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "#creating a list of my learning rates starting from very small and incrementally\n",
        "#getting bigger \n",
        "def learn_list(start, stop, step, inc):\n",
        "    #start at my smallest learning rate\n",
        "    i = start\n",
        "    #while less than stop \n",
        "    while i < stop:\n",
        "        #yeild when you get up to this line, keep this value of i, so that we can\n",
        "        #work with the data, kind of like appending but faster lol \n",
        "        yield i\n",
        "        #add step to i \n",
        "        i += step\n",
        "        #increase my stepping size \n",
        "        step += inc\n",
        "learn = list(learn_list(.001, .5, .001, .001))\n",
        "print(len(learn))\n",
        "\n",
        "#ok so Inow have 32 values for which are betweeo .001, and .5, that should give\n",
        "# us some good values from which we can see what different learning rates do.\n",
        "\n",
        "\n",
        "## boosting \n",
        "## I feel like Gradient boosting is the easiet boosting ensamble to understand\n",
        "# the grader will recall that the gradient is a directional derivative pointing\n",
        "# in the direction of greatest change, So gradient boosting is a way for us to \n",
        "# peanalize the things that we get wrong and put added weight on those \n",
        "# directional vectors to help the model \n",
        "\n",
        "#finding the mse at different learning rates \n",
        "\n",
        "#start by having two empty list for training and testing \n",
        "tr_mse = []\n",
        "ts_mse = []\n",
        "\n",
        "#for each of the different learning rates \n",
        "for i in range(len(learn)):\n",
        "    #goost with a different learning rate \n",
        "    gb = GradientBoostingRegressor(learning_rate=learn[i])\n",
        "    #use that as one of my parameters, holding the n_estimators constant \n",
        "    params = {'learning_rate':[learn[i]], \"n_estimators\":[1000],\n",
        "          \"criterion\":[\"mse\"]}\n",
        "    #grid sesrch that puppy       \n",
        "    gbgs = GridSearchCV(gb, params)\n",
        "    #fit it \n",
        "    gb.fit(X_train, y_train)\n",
        "    #predict \n",
        "    y_pred = gb.predict(X_test)\n",
        "    #find the mse \n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    #append the testing mse \n",
        "    ts_mse.append(mse)\n",
        "    tr_mse.append(gb.train_score_)\n",
        "ts_mse   \n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6165695157376515,\n",
              " 0.5078687211701748,\n",
              " 0.3444395670474103,\n",
              " 0.19297196957426185,\n",
              " 0.09008320472067537,\n",
              " 0.034813221133518625,\n",
              " 0.011154786063851898,\n",
              " 0.003090733174177704,\n",
              " 0.0008912789847329761,\n",
              " 0.00037811333800364695,\n",
              " 0.00030158821162625774,\n",
              " 0.00032917381249885215,\n",
              " 0.0002954499034382626,\n",
              " 0.00027013546595470414,\n",
              " 0.00031492918101764557,\n",
              " 0.00035559222441331205,\n",
              " 0.00034819431942551346,\n",
              " 0.00030446371709717996,\n",
              " 0.0004366710658516017,\n",
              " 0.0004606588378508595,\n",
              " 0.0004934750027246936,\n",
              " 0.0005280753553196214,\n",
              " 0.0005385730669656145,\n",
              " 0.0006994877636108355,\n",
              " 0.0008167117334516816,\n",
              " 0.0008398101849657798,\n",
              " 0.0009256246024758087,\n",
              " 0.0013732750177872248,\n",
              " 0.0014040124975702307,\n",
              " 0.001509317946710854,\n",
              " 0.0018467508669433876,\n",
              " 0.001781998169727051]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmg9k4jKS9l5",
        "colab_type": "code",
        "outputId": "09e9c6da-fc2e-4631-f748-3bce3e1bf296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# gb = GradientBoostingRegressor()\n",
        "# #use that as one of my parameters, holding the n_estimators constant \n",
        "# params = {'learning_rate':[learn[i]], \"n_estimators\":[1000],\n",
        "#           \"criterion\":[\"mse\"]}\n",
        "# #grid sesrch that puppy       \n",
        "# gbgs = GridSearchCV(gb, params)\n",
        "# #fit it \n",
        "# gb.fit(X_train, y_train)\n",
        "matrix = gb.train_score_\n",
        "matrix.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnVpcbL6c5lh",
        "colab_type": "code",
        "outputId": "6231e5c5-665c-4c8c-c345-071082e1aada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(ts_mse, label = \"testing\")\n",
        "plt.plot(matrix, label = \"training\")\n",
        "plt.title(\"learning curve for our testing\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWUY40M3MMuL/A87Qa05lETXMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-BWWOiPv-Pj",
        "colab_type": "code",
        "outputId": "f196cdfb-ed2b-4f62-f9d4-3f275b1a9213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "gb = GradientBoostingRegressor(learning_rate=learn[1])\n",
        "params = {'learning_rate':[learn[1]], \"n_estimators\":[1000],\n",
        "          \"criterion\":[\"mse\"]}\n",
        "\n",
        "gbgs = GridSearchCV(gb, params)\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "#y_pred = gb.predict(X_test)\n",
        "#mse = mean_squared_error(y_test, y_pred)\n",
        "#ts_mse.append(mse)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
              "                          learning_rate=0.002, loss='ls', max_depth=3,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                          n_iter_no_change=None, presort='auto',\n",
              "                          random_state=None, subsample=1.0, tol=0.0001,\n",
              "                          validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjBJXmZCMPMh",
        "colab_type": "text"
      },
      "source": [
        "In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.  \n",
        "(a) Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median. Should we continue using gas mileage as a predictor if this is our outcome?  \n",
        "(b) Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results.  \n",
        "(c) Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results. What parameters yield the best results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HTCT_OeMPMh",
        "colab_type": "code",
        "outputId": "c33e1153-49d5-48f7-b495-18837d2eda47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "auto = pd.read_csv(\"/content/drive/My Drive/Auto.csv\")\n",
        "auto.columns"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
              "       'acceleration', 'year', 'origin', 'name'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWezLCfvTGh_",
        "colab_type": "code",
        "outputId": "9c7fb816-9c57-4527-e3f7-2f337687c9ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "abv_avg = []\n",
        "for i in range(len(auto)):\n",
        "    if auto.mpg[i] >= np.median(auto.mpg):\n",
        "        abv_avg.append(1)\n",
        "    else:\n",
        "        abv_avg.append(0)\n",
        "auto['abv_avg'] = abv_avg\n",
        "\n",
        "'''should we continue to use gas mileage as a predictor?\n",
        "-- it depends on if we want an unbiased estimator.  Obviously the mpg of the \n",
        "car will be highly corrleated with being above or below average.  The model\n",
        "will preform better in prediction with MPG is included in the dataset '''\n",
        "auto.columns"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
              "       'acceleration', 'year', 'origin', 'name', 'abv_avg'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96gsVBoPTMCl",
        "colab_type": "code",
        "outputId": "b7d94d96-eba8-4e9f-e413-425784dd18ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#trying to see if it is worth using get_dummies on the auto.name column \n",
        "auto.name.nunique()\n",
        "#ok so there are 304 unique values, and I don't want to use 304 indicator variables \n",
        "#on this so I will just drop the column and only select numerical data to do\n",
        "# the SVM model \n",
        "\n",
        "#what is the orgin column?\n",
        "auto.origin.nunique()\n",
        "#there are only three outcomes, but they are ama"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-17yNMGVFpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auto.drop(axis = 1, columns = [\"name\"], inplace = True)\n",
        "\n",
        "#selecting the data we want \n",
        "data = auto.iloc[:, :-1]\n",
        "# making sure i don't get weird missing values / strings \n",
        "data = data.select_dtypes(include = [np.number])\n",
        "y = auto.abv_avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icrzDHynWvn0",
        "colab_type": "code",
        "outputId": "0f40fa63-9f0e-4366-982b-4e660aa810c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "'''\n",
        "Ok, the grader will recall that the goal of Support Vector Machines, SVM, is to\n",
        "see if there are natural clusters in the data, to classify a data point based\n",
        "on its vector from the orgin.  To do so SVM creates a hyper parameter between\n",
        "the two most \"extreme\" points in a natural cluster. (the two closest data points\n",
        "to the other group).  In two dimentions, the \"hyperplane\" is a vector, splitting \n",
        "the two groups. \n",
        "\n",
        "However if you can split groups linearly and there is no cross over, 1 that\n",
        "is not very interesting and second, and they are not naturally occuring phenomina\n",
        "so to get around this, and classify what we know will be eventual cross over, we \n",
        "introduce hyper parameter C. \n",
        "\n",
        "The idea of the hyper parameter is fiding a distance between a max margin\n",
        "classifier and limiting the number of margin violations.  The smaller the value\n",
        "of C, the more margin violations, but the larger distance between the hyper\n",
        "plane and the SVM.  Inversely, the higher value, the less margin violations, but \n",
        "the the smaller the distance between the hyper pland and the SV. \n",
        "\n",
        "Dr Fransen would note the higher the C is, the more variance, and reducing C\n",
        "leads to more bias. \n",
        "\n",
        "We will find C using cross validation that follows \n",
        "-----------------------------------------------------------------------------\n",
        "Another trick for SVM\n",
        "SO mathmatically, what is going on when creating the Support Vectors is that you\n",
        "are simply taking the inner product of the observations.  A quick recap, the \n",
        "inner product is very much related to the dot product and measure similarities \n",
        "of vectores and allows for vector multiplication resulting in a scaler value. It\n",
        "must meet a few more requierments and be positive definate and stuff, like that,\n",
        "I don't remember the 4 conditions but thats the idea. \n",
        "\n",
        "The kernel trick we can use will do the following, allow the model to see if a \n",
        "linear form, polynomial form or radial form will be better in classification.  \n",
        "In essence, what we are allowing the model to do, in higher dimentions, it may \n",
        "be more efficient to raise the scaler values to a power, than to just add them \n",
        "all up, or for Radial, it might be the case that two observations are very far\n",
        "apart, so we want to get the exponent of a large negitive number which makes\n",
        "the distance scaled to a very small number, this way the observations that are \n",
        "extraneous have less weight. Think loss function but not really, but same kind\n",
        "of idea... Gamma is the radial parameter > 0 \n",
        "\n",
        "Anyway, thats what we are going to put into our param_dict, so if you got this \n",
        "far thanks for reading my novel.\n",
        "'''\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y, \n",
        "                                                    test_size = .33)\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC(kernel = \"linear\")\n",
        "\n",
        "#creating dictionary\n",
        "sv_params = {\"C\": [0.03, 0.06, .09, .12, 0.15], \n",
        "             #\"kernel\": ['linear'], \n",
        "             'gamma': [0.3, .6, .9, 1.2, 1.5]}\n",
        "\n",
        "\n",
        "gridsearch = GridSearchCV(svc, \n",
        "                          sv_params, \n",
        "                          scoring='f1')\n",
        "\n",
        "gridsearch.fit(X_train, y_train)\n",
        "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
        "\n",
        "y_pred = gridsearch.predict(X_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params: {'C': 0.03, 'gamma': 0.3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJpNAawiXLVx",
        "colab_type": "code",
        "outputId": "276d3e70-e197-41a1-83c1-afbc62f7fbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "#getting accuracy metrics \n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# cool so this is classifying very well, high precision and accuracy, only one\n",
        "#miss, that's not bad "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        60\n",
            "           1       1.00      1.00      1.00        72\n",
            "\n",
            "    accuracy                           1.00       132\n",
            "   macro avg       1.00      1.00      1.00       132\n",
            "weighted avg       1.00      1.00      1.00       132\n",
            "\n",
            "[[60  0]\n",
            " [ 0 72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4taoLqEgH5i-",
        "colab_type": "code",
        "outputId": "dd714b38-0e51-4de4-c91a-2a23e4c450e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#repeating for RBF kernel \n",
        "\n",
        "svc = SVC(kernel = \"rbf\")\n",
        "sv_params = {\"C\": [0.03, 0.06, .09, .12, 0.15], \n",
        "             #\"kernel\": ['rbf'],\n",
        "             'gamma': [0.3, .6, .9, 1.2, 1.5]}\n",
        "\n",
        "\n",
        "gridsearch = GridSearchCV(svc, \n",
        "                          sv_params, \n",
        "                          scoring='f1'\n",
        "                          )\n",
        "\n",
        "gridsearch.fit(X_train, y_train)\n",
        "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
        "#getting accuracy metrics \n",
        "\n",
        "y_pred = gridsearch.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "#Not so good, not one positive was picked correctly, and every one was picked \n",
        "#incorrectly,... At first I thought I was doing something wrong, but the more \n",
        "# i look at it the more it makes sense. We expect there to be a linear relationship\n",
        "#between the mpg and the other components of a car, weight, acceleration, mpg, \n",
        "# so usint radians or circles to classify the data is not likely to translate \n",
        "# well because the nature of the data is inherantly linear. "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params: {'C': 0.03, 'gamma': 0.3}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      1.00      0.62        60\n",
            "           1       0.00      0.00      0.00        72\n",
            "\n",
            "    accuracy                           0.45       132\n",
            "   macro avg       0.23      0.50      0.31       132\n",
            "weighted avg       0.21      0.45      0.28       132\n",
            "\n",
            "[[60  0]\n",
            " [72  0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtYWrGrCH9bw",
        "colab_type": "code",
        "outputId": "e6b92252-5531-4c89-da98-113a798b0b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "#repeating for polynomial kernel \n",
        "\n",
        "svc = SVC(kernel = \"poly\", degree = 2)\n",
        "sv_params = {\"C\": [0.03, 0.06, .09, .12, 0.15], \n",
        "             #\"kernel\": ['precomputed'],\n",
        "             'gamma': [0.3, .6, .9, 1.2, 1.5]}\n",
        "\n",
        "\n",
        "gridsearch = GridSearchCV(svc, \n",
        "                          sv_params, \n",
        "                          scoring='f1',\n",
        "                          cv = 3\n",
        "                          )\n",
        "\n",
        "gridsearch.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gridsearch.predict(X_test)\n",
        "#print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
        "#getting accuracy metrics \n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "#also classifying very accurately. Just two wrong, so it looks like there is a \n",
        "# very linear relationship between the clusters of above and below average MPG\n",
        "# this one just got one extra wrong than the linear model, but still really good\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.98        60\n",
            "           1       0.99      0.97      0.98        72\n",
            "\n",
            "    accuracy                           0.98       132\n",
            "   macro avg       0.98      0.98      0.98       132\n",
            "weighted avg       0.98      0.98      0.98       132\n",
            "\n",
            "[[59  1]\n",
            " [ 2 70]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miowIkbBPcUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f3550bac-2089-4e2b-ea74-a402ab5ab85f"
      },
      "source": [
        "## Questions about the project \n",
        "\n",
        "#whith hom are you colaberating \n",
        "'''nobody'''\n",
        "#what is the research question your project will be answering?\n",
        "'''Can we effectively teach the computer to preduct NCAA march madness and \n",
        "what are the most important variables if we can do so'''\n",
        "#what data are you using?\n",
        "''' data primarily from Kaggle, they did a competition to do this a year ago'''\n",
        "teams = pd.read_csv(\"/content/drive/My Drive/Teams.csv\")\n",
        "teams.describe()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TeamID</th>\n",
              "      <th>FirstD1Season</th>\n",
              "      <th>LastD1Season</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>364.000000</td>\n",
              "      <td>364.000000</td>\n",
              "      <td>364.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1282.500000</td>\n",
              "      <td>1988.192308</td>\n",
              "      <td>2017.184066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>105.221988</td>\n",
              "      <td>7.126666</td>\n",
              "      <td>4.573449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1101.000000</td>\n",
              "      <td>1985.000000</td>\n",
              "      <td>1985.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1191.750000</td>\n",
              "      <td>1985.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1282.500000</td>\n",
              "      <td>1985.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1373.250000</td>\n",
              "      <td>1985.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1464.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "      <td>2018.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            TeamID  FirstD1Season  LastD1Season\n",
              "count   364.000000     364.000000    364.000000\n",
              "mean   1282.500000    1988.192308   2017.184066\n",
              "std     105.221988       7.126666      4.573449\n",
              "min    1101.000000    1985.000000   1985.000000\n",
              "25%    1191.750000    1985.000000   2018.000000\n",
              "50%    1282.500000    1985.000000   2018.000000\n",
              "75%    1373.250000    1985.000000   2018.000000\n",
              "max    1464.000000    2014.000000   2018.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGV1PWdui2CD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "114fc71f-f375-43ec-ee60-ddb676391814"
      },
      "source": [
        "\n",
        "power_seed = pd.read_csv(\"/content/drive/My Drive/PowerSeeds.csv\")\n",
        "power_seed.describe()\n",
        "#will have to merge these tables and various others in the future "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>PowerSeed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>960.000000</td>\n",
              "      <td>960.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2010.000000</td>\n",
              "      <td>32.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.322746</td>\n",
              "      <td>18.482582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2003.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2006.000000</td>\n",
              "      <td>16.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2010.000000</td>\n",
              "      <td>32.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2014.000000</td>\n",
              "      <td>48.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2017.000000</td>\n",
              "      <td>64.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Season  PowerSeed \n",
              "count   960.000000  960.000000\n",
              "mean   2010.000000   32.500000\n",
              "std       4.322746   18.482582\n",
              "min    2003.000000    1.000000\n",
              "25%    2006.000000   16.750000\n",
              "50%    2010.000000   32.500000\n",
              "75%    2014.000000   48.250000\n",
              "max    2017.000000   64.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPMhErlMj8_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4e4083c9-6563-4ef7-d57a-a5e10aca18ab"
      },
      "source": [
        "#what machine learning methods will you use?\n",
        "\n",
        "'''\n",
        "off the top of my head I think I will use a random forest and a gradient boosting \n",
        "to figure out to see if the higher seed won or lost. We will use dummy variables for \n",
        "power 5 conference, it is widely accepted that power 5 conferences are stronger, because \n",
        "they get better recruits, have more money, so we can use a conference dummy \n",
        "variable to show the effect of being in a power 5 conference on the effect of\n",
        "winning in march. \n",
        "''' "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\noff the top of my head I think I will use a random forest and a gradient boosting \\nto figure out to see if the higher seed won or lost. We will use dummy variables for \\npower 5 conference, it is widely accepted that power 5 conferences are stronger, because \\nthey get better recruits, have more money, so we can use a conference dummy \\nvariable to show the effect of being in a power 5 conference on the effect of\\nwinning in march. \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8PPb3DrkNkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}